---
title: "Spam Classifier"
format:
  html:
    embed-resources: true
execute:
  echo: false
  warning: false
  
---

## Introduction
```{r}
library(tidyverse) # stuff
library(DisplayR) # themeing
library(here) # file paths
library(ggtext) # nice text rendering
library(DT) # viewing data 
library(patchwork) # stitching sentiment plots
library(ggiraph) # interactive loss plot, may just use ggplot instead
```


In the [Peaks and Pits](peaks_pits_workflow.qmd){target="_blank"}, and [Conversation Landscape](conversation_landscape.qmd){target="_blank"} case studies, we walked through some of our fundamental project offerings. Here we walk through a different type of project, where the goal of the research is slightly more abstract - to product some thought leadership-style guidance of data quality on social. The first step of the project was to tackle spam, at scale.

## Spam Classifier

The vast majority of our work is centred around answering research questions which are proscribed to us by stakeholders. To achieve this aim, we try to extract, understand, and represent the organic opinions of real people. Our work is good in proportion to how accurately we can do this: the more organic & accurate data that we extract, the better our answers, and vice versa.

As practitioners we have all lived through the frustration of arriving at the end of a research project only to find an overlooked cluster of spam is skewing our results; sending us back to the beginning of the process to clean the data and repeat the analysis. When this happens it can be draining on both time and motivation, and if we're pressed for either than we may end up producing work that isn't accurate, or isn't as accurate as it could be.

## Motivation
```{r, sentiment_charts}
# data is taken from the spam workflow, hardcoded to avoid syncing file paths across users to the data_science_project_workflow
sentiment_data <- tribble(
  ~ spam_label, ~sentiment, ~n, ~percent,
  "Not Spam", "NEGATIVE", 193014, 38.9,
  "Not Spam", "NEUTRAL", 201439, 40.6,
  "Not Spam", "POSITIVE", 102299, 20.6,
  "Spam", "NEGATIVE", 32590, 10.8,
  "Spam", "NEUTRAL", 152302, 50.6,
  "Spam", "POSITIVE", 115850, 38.5
) %>%
  mutate(sentiment = 
           factor(sentiment, levels = c("NEGATIVE", "NEUTRAL", "POSITIVE")))

sentiment_colours <- c(
    "NEGATIVE" = "darkred",
    "POSITIVE" = "darkgreen",
    "NEUTRAL" = "grey60"
  )

sentiment_unclassified <- sentiment_data %>%
  ggplot(aes(x= "", y = percent, fill = sentiment)) + 
  geom_bar(stat = "identity") +
    geom_text(
    aes(label = round(percent, 1)),
    position = ggplot2::position_stack(0.5),
    color = "white"
    ) +
  dr_theme_capture() +
  coord_flip() +
  scale_fill_manual(values = sentiment_colours)+
  labs(x = NULL, y = NULL, title = "Overall Dataset", fill = NULL)

sentiment_classified <- sentiment_data %>%
  ggplot(aes(x = percent, y = spam_label, fill = sentiment)) +
  geom_col() + 
  geom_text(
    aes(label = round(percent, 1)),
    position = ggplot2::position_stack(0.5),
    color = "white"
    ) +
  dr_theme_capture() +
  scale_fill_manual(values = sentiment_colours) +
  labs(x = "Percent (%)", y = NULL, fill = NULL, title = "Classified Dataset") +
  theme(legend.position = "none")
```

Take the following sentiment distribution:
```{r, sentiment_unclassified_plot}
#| fig-cap: "Figure 1. Sentiment Distribution"
#| fig-height: 2
#| fig-width: 6
sentiment_unclassified
```

[Briefly, Simpson's paradox demonstrates how important differences between groups can be obscured by aggregation.]{.aside}

The chart indicates an approximately even split between Positive and Negative, and a large proportion of Neutral. This distribution is common and unremarkable. We could make some reasonable inferences about the data based on this distribution. However, lurking under the surface is a problem similar to [Simpson's Paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox). 

For example, if we separate our dataset into groups of 'Spam' and 'Not Spam', the balance between Positive and Negative disappears entirely. The 'Spam' section of the dataset has ~4x as many Positive mentions than Negative, whereas the 'Not Spam' section has ~2x as many Negative than Positive; an ~8x swing. 

```{r, sentiment_classified_plot}
#| fig-cap: "Figure 1.1. Removing 'Spam' has a dramatic effect on the distribution of sentiment in a dataset."
#| fig-width: 8
#| fig-height: 2
sentiment_classified 
```

Practically speaking, failure to remove spam could be the difference between deriving an accurate picture of reality and not, or, a correctly-timed strategic intervention and continuing as-is, assuming everything is ok.

We also want to:

  1. Reduce time spent cleaning data 
  2. Increase consistency of output
  3. Try our best to comply with the assumptions of the algorithms we use, e.g. for clustering or topic modelling
  
## What is Spam?

Broadly speaking, we can define spam as **'Unwanted, irrelevant, or unsolicited mentions sent to large audiences.'** However, whether something is unwanted or irrelevant is open to interpretation and varies with context; so precisely stating what each of these words means is difficult. Despite this difficulty, we will mostly tend to agree with one another when we actually see spam. There is a certain 'know it when I see it' aspect of spam. 

It should be quite clear which of the following two documents is spam and which is not:

:::: {.columns}

::: {.column}
> "Need help with online exams, assignments, research projects, or dissertations? Look no further! I can assist with proofreading, personal statements, and more. Let's tackle #MachineLearning, #DataScience, #Python, #Cybersecurity, #BigData, #AI, #IoT, #DeepLearning, #NLP together!"
:::

::: {.column}
> “I think one of the big open questions is whether anyone will challenge the order in court. It uses presidential emergency powers to require red-teaming of foundation models. Initially that may only affect companies like OpenAI and Anthropic that have been asking for regulation.”
:::

::::

## Methodology

With our loose definition of 'Spam' in place, we used its 'know it when I see it' property to curate a corpus of 'Spam' and 'Not Spam' mentions. We then used a Transfer Learning^[Taking a model which has already been trained for general purposes, and training it on a specific task.] approach to fine-tune a foundational Language Model to classify data into 'Spam' and 'Not Spam'.

When labelling 'Spam' and 'Not Spam' we added optional labels as the need arose. For example, we saw a lot of spam regarding wallet scams, memecoins, crypto price updates, sustainabile coins, and a whole lot more relating to cryptocurrencies, so we added a label for 'Crypto'. 

::: {.callout-warning collapse='true'}
## Creating Labels on the Fly

In a project like this, creating new labels as we went was inevitable, because we were discovering/learning about the nature of Spam as we labelled, and as part of the research brief we were to create a taxonomy of spam - meaning we needed to identify groups of spam from the data. 

That said, if we introduce a new label at data point 1,000, are we *really* going to go back and check which of the previous 1,000 data points may fit into that label? The answer is most likely no. **Depending on what you're using these labels for, this may or may not be harmful.** For our cases, the additional labels were of secondary importance, so we could tolerate them not being consistent throughout the dataset. We also set out to identify cases off LLM-generated [slop](https://en.wikipedia.org/wiki/Slop_(artificial_intelligence)) ^["...slop generated by large language models, written by no one to communicate nothing.", Robyn Speer, WordFreq maintainer]

To be clear, the important label for the model was 'Spam' vs 'Not Spam'.
:::

reating the additional labels served two main purposes:

1. Priors for the Taxonomy creation
2. Discrete variables to systematically improve model performance

We were delivering the Taxonomy as a scoped commitment, and we will talk about 2. in more detail in the [Systematically Improving a Model Section](#systematically-improving-a-model).

When selecting models to test we needed to consider the following constraints:

- Weights are open-source 
- Permissive License
- Capable of performing the task at hand
- Can be fine-tuned on consumer hardware 
- Simple to deploy and run inference at scale

We tested a variety of candidate models and ultimately opted for a fine-tune of 'Roberta-base' as it performed the best in all metrics. 

[TODO: get gt table for results]

## Corpus

Each tab houses a selection of the texts which have been assigned that label inside the corpus, click through the tabs to start building a mental model for the type of data that will be removed by the classifier.

::: {.callout-warning}
It's crucial when using the model to check what you are removing - do not blindly

```{r}
corpus_samples <- read_csv(here("data/handbook_samples.csv"))
split_names <- sort(unique(corpus_samples$split_name))
corpus_samples_splits <- corpus_samples %>% group_split(split_name) %>%
  setNames(split_names)

view_dt <- function(data) {
  data %>%
    DT::datatable(options = list(pageLength = 20, searching = TRUE))
}
```

::: {.panel-tabset}

## Promotion

Something worth detailing here is that in the early labels, before it was clear that people sharing their AI Generations might be their own type of spam, I was considering links to AI images as a form of promotion. If the goal was to separate promotions from AI generations this would be problematic.

However, this corpus is for separating 'Spam' from 'Not Spam', so we can accept some fuzziness between categories.
```{r, promotion}
#| column: page-right
corpus_samples_splits$promo %>%
  view_dt()
```


## Slop

It is not always possible to tell whether something was written by an AI, a person, or a person using AI. Some slop, however, is much more obvious than others.

```{r, slop}
#| column: page-right
corpus_samples_splits$slop %>%
  view_dt()
```

## Crypto

```{r, crypto}
#| column: page-right
corpus_samples_splits$crypto %>%
  view_dt()
```

## Article Link

```{r, article_link}
#| column: page-right
corpus_samples_splits$article %>%
  view_dt()
```

## SEO
SEO will tend to be generic content (sometimes including links) with excessive use of keywords, or hashtags to boost content visibility.
```{r, seo}
#| column: page-right
corpus_samples_splits$seo %>%
  view_dt()
```

## Announcement

```{r, announcement}
#| column: page-right
corpus_samples_splits$announcement %>%
  view_dt()
```

## Event
```{r, event}
#| column: page-right
corpus_samples_splits$event %>%
  view_dt()
```

## Report
Quite similar to 'article_link' except reports are mainly the content of reports, they have often been truncated (ending in ...)  by Sprinklr's scrapers. They are potentially tricky to deal with because they will often not look like spam. 
```{r, report}
#| column: page-right
corpus_samples_splits$report %>%
  view_dt()
```


## Quote
We started out collecting these because quotes can have unwanted effects in many of our downstream tasks, and then settled on texts where people are just parroting a quote being Spam. I strongly suspect that some of these quotes will not be spam, and should be revisited.
```{r, quote}
#| column: page-right
corpus_samples_splits$quote %>%
  view_dt()
```

:::

```{r, samples_for_handbook}
#| column: page-right
#| eval: false
function(data){
  select(data, "spam_corpus_id", "text", "classification_label") %>%
    slice(1:20) 
}

corpus %>%
  filter(str_detect(classification_label, "promotion")) %>%
  tibble_quick_select() 

promo <- corpus %>%
  filter(str_detect(classification_label, "promotion")) %>%
  tibble_quick_select() 
slop <- corpus %>% 
  filter(str_detect(classification_label, "slop"), str_detect(text, "🚀|🔐|🌐|✨")) %>%
  tibble_quick_select() 
crypto <- corpus %>% 
  filter(str_detect(classification_label, "crypto")) %>%
  sample_n(20) %>%
  tibble_quick_select()
article <- corpus %>%
  filter(str_detect(classification_label, "article")) %>%
  tibble_quick_select()
seo <- corpus %>% 
  filter(str_detect(classification_label, "seo")) %>%
  tibble_quick_select()
announcement <- corpus %>%
  filter(str_detect(classification_label, "announcement")) %>%
  tibble_quick_select()
event <- corpus %>%
  filter(str_detect(classification_label,"event" )) %>%
  tibble_quick_select()
report <- corpus %>%
  filter(str_detect(classification_label, "report")) %>%
  tibble_quick_select()
quote <- corpus %>%
  filter(str_detect(classification_label, "quote")) %>%
  tibble_quick_select()

splits <- list(promo, slop, crypto, article, seo, announcement, event, report, quote)
names <- c("promo", "slop", "crypto", "article", "seo", "announcement", "event", "report", "quote")

map2(splits, names, ~ .x %>% mutate(split_name = .y)) %>%
  bind_rows() %>%
  write_csv(here("data/handbook_samples.csv"))

```
