[
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "11  Data Cleaning",
    "section": "",
    "text": "11.1 Dataset-level cleaning\nGoal: Ensure the dataset as a whole is relevant and of high quality\nThe main steps that we take for this level of cleaning is spam removal, uninformative content removal and deduplication",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data_cleaning.html#dataset-level-cleaning",
    "href": "data_cleaning.html#dataset-level-cleaning",
    "title": "11  Data Cleaning",
    "section": "",
    "text": "11.1.1 Spam Removal\nWe use the term “spam” quite loosely in our data pre-processing workflows. Whilst the strict definition of “spam” could be something like “unsolicited, repetitive, unwanted content”, we can think of it more broadly any post that displays irregular posting patterns or is not going to provide analytical value to our research project.\n\n11.1.1.1 Hashtag filtering\nThere are multiple ways we can identify spam to remove it. The simplest is perhaps something like hashtag spamming, where an excessive number of hashtags, often unrelated to the content of the post, can be indicative of spam.\nWe can identify posts like this by counting the number of hashtags, and then filtering out posts that reach a certain (subjective) threshold.\n\ncleaned_data &lt;- data %&gt;% \n  mutate(extracted_hashtags = str_extract_all(message_column, \"#\\\\S+\"),\n         number_of_hashtags = lengths(extracted_hashtags)) %&gt;% \n  filter(number_of_hashtags &lt; 5)\n\nIn the example above we have set the threshold to be 5 (so any post that has 5 or more hashtags will be removed), however whilst this is a valid starting point, it is highly recommend to treat each dataset uniquely in determining which threshold to use.\n\n\n11.1.1.2 Spam-grams\nOften-times spam can be identified by repetitive posting of the same post, or very similar posts, over a short period of time.\nWe can identify these posts by breaking down posts into n-grams, and counting up the number of posts that contain each n-gram. For example, we might find lots of posts with the 6-gram “Click this link for amazing deals”, which we would want to be removed.\nTo do this, we can unnest our text data into n-grams (where we decide what value of n we want), count the number of times each n-gram appears in the data, and filter out any post that contains an n-gram above this filtering threshold.\nThankfully, we have a function within the LimpiaR package called limpiar_spam_grams() which aids us with this task massively. With this function, we can specify the value of n we want and the minimum number of times an n-gram should occur to be removed. We are then able to inspect the different n-grams that are removed by the function (and their corresponding post) optionally changing the function inputs if we need to be more strict or conservative with our spam removal.\n\nspam_grams &lt;- data %&gt;% \n  limpiar_spam_grams(text_var = message_column,\n                     n_gram = 6,\n                     min_freq = 6)\n\n# see remove spam_grams\nspam_grams %&gt;% \n  pluck(\"spam_grams\")\n\n# see deleted posts\nspam_grams %&gt;% \n  pluck(\"deleted\")\n\n# save 'clean' posts\nclean_data &lt;- spam_grams %&gt;% \n  pluck(\"data\")\n\n\n\n11.1.1.3 Filter by post length\nDepending on the specific research question or analysis we will be performing, not all posts are equal in their analytical potential. For example, if we are investigating what specific features contribute to the emotional association of a product with a specific audience, a short post like “I love product” (three words) won’t provide the level of detail required to answer the question.\nWhile there is no strict rule for overcoming this, we can use a simple heuristic for post length to determine the minimum size a post needs to be before it is considered informative. For instance, a post like “I love product, the features x and y excite me so much” (12 words) is much more informative than the previous example. We might then decide that any post containing fewer than 10 words (or perhaps 25 characters) can be removed from downstream analysis.\nOn the other end of the spectrum, exceedingly long posts can also be problematic. These long posts might contain a lot of irrelevant information, which could dilute our ability to extract the core information we need. Additionally, long posts might be too lengthy for certain pipelines. Many embedding models, for example, have a maximum token length and will truncate posts that are longer than this, meaning we could lose valuable information if it appears at the end of the post. Also, from a practical perspective, longer posts take more time to analyse and require more cognitive effort to read, especially if we need to manually identify useful content (e.g. find suitable verbatims).\n\n# Remove posts with fewer than 10 words\ncleaned_data &lt;- data %&gt;% \n  filter(str_count(message_column, \"\\\\w+\") &gt;= 10)\n\n# Remove posts with fewer than 25 characters and more than 2500 characters\ncleaned_data &lt;- data %&gt;% \n  filter(str_length(message_column) &gt;= 25 & str_length(message_column) &lt;= 2500)\n\n\n\n\n11.1.2 Deduplication\nWhile removing spam often addresses repeated content, it’s also important to handle cases of exact duplicates within our dataset. Deduplication focuses on eliminating instances where entire data points, including all their attributes, are repeated.\nA duplicated data point will not only have the same message_column content but also identical values in every other column (e.g., universal_message_id, created_time, permalink). This is different from spam posts, which may repeat the same message but will differ in attributes like universal_message_id and created_time.\nAlthough the limpiar_spam_grams() function can help identify spam through frequent n-grams, it might not catch these exact duplicates if they occur infrequently. Therefore, it is essential to use a deduplication step to ensure we are not analysing redundant data.\nTo remove duplicates, we can use the distinct() function from the dplyr package, ensuring that we retain only unique values of universal_message_id. This step guarantees that each post is represented only once in our dataset.\n\ndata_no_duplicates &lt;- data %&gt;% \n  distinct(universal_message_id, .keep_all = TRUE)",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data_cleaning.html#document-level-cleaning",
    "href": "data_cleaning.html#document-level-cleaning",
    "title": "11  Data Cleaning",
    "section": "11.2 Document-level cleaning",
    "text": "11.2 Document-level cleaning\nGoal: Prepare each individual document (post) for text analysis.\nAt a document-level (or individual post level), the steps that we take are more small scale. The necessity to perform each cleaning step will depend on the downstream analysis being performed, but in general the different steps that we can undertake are:\n\n11.2.1 Remove punctuation\nOften times we will want punctuation to be removed before performing an analysis because they tend to not be useful for text analysis. This is particularly the case with more ‘traditional’ text analytics, where an algorithm will assign punctuation marks a unique numeric identify just like a word. By removing punctuation we create a cleaner dataset by reducing noise.\n\n\n\n\n\n\nWarning on punctuation\n\n\n\n\n\nFor more complex models, such as those that utilise word or sentence embeddings, we often keep punctuation in. This is because punctuation is key to understanding a sentences context (which is what sentence embeddings can do).\nFor example, there is a big difference between the sentences “Let’s eat, Grandpa” and “Let’s eat Grandpa”, which is lost if we remove punctuation.\n\n\n\n\n\n11.2.2 Remove stopwords\nStopwords are extremely common words such as “and,” “the,” and “is” that often do not carry significant meaning. In text analysis, these words are typically filtered out to improve the efficiency of text analytical models by reducing the volume of non-essential words.\nRemoving stopwords is particularly useful in our projects for when we are visualising words, such as a bigram network or a WLO plot, as it is more effective if precious informative space on the plots is not occupied by these uninformative terms.\n\n\n\n\n\n\nWarning on stopword removal\n\n\n\n\n\nSimilarly to the removal of punctuation, for more complex models (those that utilise word or sentence embeddings) we often keep stopwords in. This is because these stopwords can be key to understanding a sentences context (which is what sentence embeddings can do).\nFor example, imagine if we removed the stopword “not” from the sentence “I have not eaten pizza”- it would become “I have eaten pizza” and the whole context of the sentence would be different.\nAnother time to be aware of stopwords is if a key term related to a project is itself a stopword. For example, the stopwords list SMART treats the term “one” as a stopword. If we were studying different Xbox products, then the console “Xbox One” would end up being “Xbox” and we would lose all insight referring to that specific model. For this reason it is always worth double checking which stopwords get removed and whether it is actually suitable.\n\n\n\n\n\n11.2.3 Lowercase text\nConverting all text to lowercase standardises the text data, making it uniform. This helps in treating words like “Data” and “data” as the same word, and is especially useful when an analysis requires an understanding of the frequency of a term (we rarely want to count “Data” and “data” as two different things) such as bigram networks.\n\n\n11.2.4 Remove mentions\nMentions (e.g., @username) are specific to social media platforms and often do not carry significant meaning for text analysis, and in fact may be confuse downstream analyses. For example, if there was a username called @I_love_chocolate, upon punctuation remove this might end up confusing a sentiment algorithm. Removing mentions therefore helps in focusing on the actual content of the text.\n\n\n\n\n\n\nRetaining mentions, sometimes\n\n\n\n\n\nWe often perform analyses that involve network analyses. For these, we need to have information of usernames because they appear when users are either mentioned or retweeted. In this case we do not want to remove the @username completely, but rather we can store this information elsewhere in the dataframe.\nHowever, broadly speaking if the goal is to analyse the content/context of a paste, removing mentions is very much necessary.\n\n\n\n\n\n11.2.5 Remove URLs\nURLs in posts often point to external content and generally do not provide meaningful information for text analysis. Removing URLs helps to clean the text by eliminating these irrelevant elements.\n\n\n11.2.6 Remove emojis/special characters\nEmojis and special characters can add noise to the text data. While they can be useful for certain analyses (like emoji-specific sentiment analysis - though we rarely do this), they are often removed to simplify text and focus on word-based analysis.\n\n\n11.2.7 Stemming/Lemmatization\nStemming and lemmatization are both techniques used to reduce words to their base or root form and act as a text normalisation technique.\nStemming trims word endings to their most basic form, for example changing “clouds” to “cloud” or “trees” to “tree”. However, sometimes stemming reduces words to a form that doesn’t make total sense such as “little” to “littl” or “histories” to “histori”.\nLemmatization considers the context and grammatical role when normalising words, producing dictionary definition version of words. For example “histories” would become “history”, and “caring” would become “car” (whereas for stemming it would become “car”).\nWe tend to use lemmatization over stemming- despite it being a bit slower due to a more complex model, the benefit of lemmitization outweighs this. Similar to lowercasing the text, lemmitization is useful when we need to normalise text where having distinct terms like “change”, “changing”, “changes”, and “changed” isn’t necessary and just “change” is suitable.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data_cleaning.html#conclusion",
    "href": "data_cleaning.html#conclusion",
    "title": "11  Data Cleaning",
    "section": "11.3 Conclusion",
    "text": "11.3 Conclusion\nDespite all of these different techniques, it is important to remember these are not mutually exclusive, and do not always need to be performed. It may very well be the case where a specific project actually required us to mine through the URLs in social posts to see where users a linking too, or perhaps keeping text as all-caps is important for how a specific brand or product is mentioned online. Whilst we can streamline the cleaning steps by using the ParseR function above, it is always worth spending time considering the best cleaning steps for each specific part of a project. It is much better spending more time at the beginning of the project getting this right, than realising that the downstream analysis are built on dodgy foundations and the data cleaning step needs to happen again later in the project, rendering intermediate work redundant.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "calling_apis.html",
    "href": "calling_apis.html",
    "title": "12  Calling APIs",
    "section": "",
    "text": "12.1 Why Do We Use APIs?\nAs a team, our main use case for APIs is the OpenAI API, which grants us access to the advanced AI models developed by OpenAI, including the GPT (text), DALL-E (image generation), and Whisper (speech-to-text) models. One of the key advantages of using an API instead of downloading and running these models locally (or utilising open-source models) is that it allows us to leverage the computational power and optimization of the models without needing expensive hardware or vast computational resources.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Calling APIs</span>"
    ]
  },
  {
    "objectID": "calling_apis.html#openai-api-overview",
    "href": "calling_apis.html#openai-api-overview",
    "title": "12  Calling APIs",
    "section": "12.2 OpenAI API Overview",
    "text": "12.2 OpenAI API Overview\nOpenAI’s API is a REST (Representational State Transfer) API. Simply put, this allows a client (such as our program) to request data or perform actions on a server (which hosts the AI models), where it retrieves or manipulates resources (e.g., model outputs such as generated text).\n\n12.2.1 How the API Works\nOpenAI’s API works on the standard HTTP protocol, which structures communication between the client and server. In this system:\n\nEndpoints are specific paths on the server where the API can be accessed. For example, /v1/chat/completions is an endpoint that allows us to send prompts to a GPT model and receive completions.\nRequests are the actions taken by our application. We send requests with specific inputs (like text prompts), and the API processes them.\nResponses are the API’s outputs, such as text from a GPT model, an image from DALL-E, or speech-to-text conversions from Whisper.\n\n\n\n12.2.2 Understanding Tokens and Model Usage\nAPIs like OpenAI’s typically have costs associated with their usage, and this is often measured in tokens. When you input text or data into an API, it is broken down into tokens, which are individual units of language (like parts of words or characters).\n\nInput Tokens: These are the tokens sent to the API (e.g., your prompt to GPT). Every word, punctuation mark, or whitespace counts toward your input tokens.\nOutput Tokens: These are the tokens returned from the API as a response (e.g., the AI’s reply). The longer and more complex the output, the more tokens are consumed.\n\nManaging tokens is crucial because they directly impact the cost of API usage. Different models have different costs per token, with more advanced models being more expensive but often providing better results. For example, as of writing this document (October 2024), the pricing for gpt-4o-mini is $0.150/1M input tokens and $0.500/1M output tokens compared to $5.00/1M input tokens and $15.00/1M output tokens for gpt-4o. Or in other words, gpt-4o-mini is ~30x cheaper than gpt-4o! Check out the model pricing information to see the latest costs, and the model pages to see the differences in model capabilities (i.e. context windows, maximum output tokens, training data).\n\n\n\n\n\n\nTokens to Words\n\n\n\n\n\n\nA token is typically about 4 characters of English text.\n100 tokens are roughly equivalent to 75 words.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Calling APIs</span>"
    ]
  },
  {
    "objectID": "calling_apis.html#practical-use-of-openais-api",
    "href": "calling_apis.html#practical-use-of-openais-api",
    "title": "12  Calling APIs",
    "section": "12.3 Practical Use of OpenAI’s API",
    "text": "12.3 Practical Use of OpenAI’s API\nBasically, we use the API in the same way as we would use other public APIs - sign up for an account, obtain an API key, and then use the key to make API calls to specific models using HTTP requests.\n\n12.3.1 Step One - Obtain API Key and Authentication\nTo start using OpenAI’s API, you’ll need an API key for authentication. Follow these steps:\n\nGo to platform.openai.com and create an account using your SHARE email address.\nMike will add you to the “SHARE organization” within the platform, allowing you to access the set aside usage credits we have as a company.\nThen make your way to the api-keys section of the platform and click the green Create new secret key in the top corner.\n\n\n\n\nCreate new secret key by clicking the button in the top right hand corner\n\n\n\nRename the key to something useful, such as the name and number of the project that they key will be used for, and keep the OpenAI project as “Default project” and Permissions as “All”.\nYou will then be provided with the opportunity to copy the provided API key, this is the one chance you will get to obtain it- after you click off this pop up you won’t be able to view the full API key again and you’ll need to request a new one. Because of this, make sure you copy the key and add it to this private Google Sheet where the DS team keeps the API Keys. Remember that using the API costs money, so if this key is used by others we risk someone using up all of our API credits! Please see below for some more best practices relating to API key security.\n\n\n\n12.3.2 Step Two - Managing API Keys Securely\nAs outlined above, when working with APIs it’s essential to manage our API keys securely. An API key grants access to services, and if exposed, others could misuse it, leading to security breaches, unauthorised usage, or unexpected costs. Here are some key principles to follow:\n\nNever Hard-Code API Keys Avoid storing API keys directly in your code as hard-coded variables. This exposes them to anyone with access to your codebase.\nUse Environment Variables Store API keys in environment variables to keep them separate from the code. This ensures sensitive data isn’t exposed, and it’s easier to manage keys across different environments if required (development, production, etc.).\nVersion Control Precautions Make sure to add environment files that contain sensitive information (like .env or .Renviron) to .gitignore so they don’t get uploaded to version control systems like GitHub. Exposing API keys in public repositories is a common mistake, and it can be a serious security risk.\n\n\n12.3.2.0.1 Example implementations\nRStudio\n\nAdd API Key to .Renviron\n\nUse the usethis package to edit the .Renviron file where environment variables are stored. Add the API key like this:\n\nusethis::edit_r_environ(scope = \"project\")\n\nThis will open the .Renviron file in your editor. Note that scope = \"project\" scope means that the .Renviron file will be created in your specific R project folder. This means the environment variables (like your API key) will only be available when you are working inside that project. It’s a good way to keep project-specific configuration separate from other projects.\nThen add the following line to store your API key (replace your-api-key-here with the actually API key)\n\n# Write this within the .Renviron file and save it\nOPENAI_API_KEY=your-api-key-here\n\n\nAccess the API Key in your R Script\n\nYou can access the API key in your R scripts using Sys.getenv()\n\napi_key &lt;- Sys.getenv(\"OPENAI_API_KEY\")\n\nor if you need to call the API key in a function (such as BERTopicR) it could be\n\nrepresentation_openai &lt;- bt_representation_openai(fitted_model,\n                                                  documents,\n                                                  openai_model = \"gpt-4o-mini\",\n                                                  nr_repr_docs = 10,\n                                                  chat = TRUE,\n                                                  api_key = Sys.getenv(\"OPENAI_API_KEY\"))\n\n\nAdd .Renviron to .gitignore\n\nObviously this is only relevant if you are deploying a repo/project to GitHub, but we can make sure to exclude the .Renviron file to our .gitignore file\n\n# Exclude .Renviron file\n.Renviron\n\nPython\n\nCreate a .env file\n\nIn the root directory of your project, create a .env file. The best way to do this is using command line tools (touch and nano)\nWithin the terminal create an empty .env file by running\n\ntouch .env\n\nand then edit it by running\n\nnano .env\n\nand finally within the nano editor, type the following to add your API key (replace your-api-key-here with the actually API key)\n\nOPENAI_API_KEY=your-api-key-here\n\n\nUse the python-dotenv library\n\nInstall python-dotenv by running\n\npip install python-dotenv\n\n\nAccess the API Key in your script\n\nIn your Python script, load the .env file and access the API key\n\nfrom dotenv import load_dotenv\nimport os\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Access the API key\napi_key = os.getenv(\"OPENAI_API_KEY\")\n\n\nAdd .env to .gitignore\n\nSimilar to the RStudio implementation above, add .env to your .gitignore\n\n# Exclude .env file\n.env\n\n\n\n\n12.3.3 Step Three - Making Requests to the API\nTo actually make requests to the OpenAI API we use python, and specifically the official OpenAI SDK. You can install it to your python environment simply via pip by running\n\npip install openai\n\nThe documentation on-line surrounding calling the OpenAI API is extremely extensive and generally good, however the API and underlying models do get updated quite often and this can cause code to become redundant or not act as one may expect. This can be particularly unwelcome when you run a previously working script to ping the API, get charged, but don’t receive an output that is useful.\nThe simple way to call the API and obtain a ‘human-like response’ to a prompt is with this code adapted from the OpenAI API tutorial:\n\nfrom openai import OpenAI\nclient = OpenAI(api_key = OPENAI_API_KEY)\n\ncompletion = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Write a short poem about RStudio.\"}\n    ]\n)\n\nprint(completion.choices[0].message)\n\nDon’t worry about what everything means, we’ll explain this in a bit more detail below. But firstly, one thing to realise is that this code above is effectively the same as going onto the ChatGPT website and typing into the input box “You are a helpful assistant. Write a short poem about RStudio.” for the model gpt-4o-mini. So effectively this code calls the API once, with an input and receives an output from the model.\n\n12.3.3.1 Chat Completions\nTo use one of the text models, we need to send a request to the Chat Completions API containing the inputs and our API key, and receive a response containing the model’s output.\nThe API accepts inputs by the parameter messages, which is an array of message objects. Each message object has a role, either system, user, or assistant, and content.\n\nThe system message is optional and can be used to set the behaviour of the assistant\nThe user messages provide requests or comments for the assistant to respond to\nAssistant messages store previous assistant responses, but can also be written by us to give examples of desired behaviour (however note we can also provide examples within the user message- which is what we tend to do in our workflows)\n\nFor example:\n\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n  ]\n)\n\nWhilst this chat format is designed to work well with multi-turn conversations, in reality we use it for single-turn tasks without a full conversation. So we would normally have something more like:\n\nfrom openai import OpenAI\nclient = OpenAI(api_key = OPENAI_API_KEY)\n\ncompletion = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant who specialised in sentiment analysis\"},\n        {\"role\": \"user\", \"content\": \"What is the sentiment of the following text: 'I love reading this Handbook'\"}\n    ]\n)\n\nprint(completion.choices[0].message)\n\nThe response (defined as completion in the code above) of the Chat Completions API looks like the following:\n\n{\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"The sentiment of the text 'I love reading this Handbook' is positive. The use of the word 'love' indicates a strong positive emotion towards the Handbook.\",\n        \"role\": \"assistant\"\n      },\n      \"logprobs\": null\n    }\n  ],\n  \"created\": 1677664795,\n  \"id\": \"chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW\",\n  \"model\": \"gpt-4o-mini\",\n  \"object\": \"chat.completion\",\n  \"usage\": {\n    \"completion_tokens\": 26,\n    \"prompt_tokens\": 13,\n    \"total_tokens\": 39,\n    \"completion_tokens_details\": {\n      \"reasoning_tokens\": 0\n    }\n  }\n}\n\nWe can see there is a lot of information here, such as the model used and the number of input tokens. You will notice the response is a dictionary and made up of key-value pairs to help organise the relevant information. However, we are mostly focussed on the models output (that is, the assistants reply), which we can extract by running:\n\nmessage = completion.choices[0].message.content\n\n\n\n\n12.3.4 Best Practices for OpenAI’s API\n\nUse the Playground- The Playground is a tool from OpenAI that you can use to learn how to construct prompts, so that you can get comfortable using querying the model and learning how the API works. Note however that using the Playground incurs costs. While it’s unlikely you will rack up a large bill be analysing large corpora within the playground, it is still important to be mindful of usage.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Calling APIs</span>"
    ]
  },
  {
    "objectID": "calling_apis.html#to-do",
    "href": "calling_apis.html#to-do",
    "title": "12  Calling APIs",
    "section": "12.4 To Do",
    "text": "12.4 To Do\n[] Best practices\n[] Structured Outputs\n[] Cost efficiency/more detailed paramters (max_tokens etc)\n[] Batch processing\n\n\n\nCreate new secret key by clicking the button in the top right hand corner",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Calling APIs</span>"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "13  Resources",
    "section": "",
    "text": "13.1 Data Visualisation",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#data-visualisation",
    "href": "resources.html#data-visualisation",
    "title": "13  Resources",
    "section": "",
    "text": "Capture Cookbook\nR Graph Gallery",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#literate-programming",
    "href": "resources.html#literate-programming",
    "title": "13  Resources",
    "section": "13.2 Literate Programming",
    "text": "13.2 Literate Programming\n\nQuarto Guide\nQuarto Markdown Basics\nRStudio R Markdown guide\nVisual R Markdown",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#package-development",
    "href": "resources.html#package-development",
    "title": "13  Resources",
    "section": "13.3 Package Development",
    "text": "13.3 Package Development\n\nR Packages Book\npkgdown\nroxygen2\nusethis\ndevtools\ntestthat\nTidyverse Design Guide\nHappy Git with R",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#youtube-channels",
    "href": "resources.html#youtube-channels",
    "title": "13  Resources",
    "section": "13.4 YouTube Channels",
    "text": "13.4 YouTube Channels\nThese channels will help you see the power and flexibility of data analysis & science in R:\n\nJulia Silge\nDavid Robinson",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#shiny",
    "href": "resources.html#shiny",
    "title": "13  Resources",
    "section": "13.5 Shiny",
    "text": "13.5 Shiny\n\nMastering Shiny\nEngineering Enterprise-grade Shiny Applications with Golem\nOutstanding Shiny Interfaces\nModularising Shiny Apps, YT\nPresenting Golem, YT\nStyling Shiny, Joe Chen, YT",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#text-analytics",
    "href": "resources.html#text-analytics",
    "title": "13  Resources",
    "section": "13.6 Text Analytics",
    "text": "13.6 Text Analytics\n\nText Mining with R (Tidytext)\nSupervised Machine Learning for Text Analysis in R\nSpeech & Language Processing\nNatural Language Processing with Transformers - we have a physical copy floating around the office",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html",
    "href": "code_best_practices.html",
    "title": "10  Coding best practices",
    "section": "",
    "text": "10.1 Why are we here?\nAt SAMY, code is the language of both our research and our development, so it pays to invest in your coding abilities. There are many great (and many terrible) resources on learning how to code. This document will focus on practical tips on how to structure your code to reduce cognitive strain and do the best work you can.\nLet’s be clear about what coding is: coding is thinking not typing, so good coding is simply good thinking and arranging our code well will help us to think better.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#reproducible-analyses",
    "href": "code_best_practices.html#reproducible-analyses",
    "title": "10  Coding best practices",
    "section": "10.2 Reproducible Analyses",
    "text": "10.2 Reproducible Analyses\nAbove everything else, notebooks must be reproducible. What do we mean by reproducible? You and your collaborators should be able to get back to any place in your analysis simply by executing code in the order it occurs in your scripts and notebooks. Hopefully the truth of this statement is self-evident. But if that’s the case, why are we talking about it?\nFor some projects you’ll get away with a folder structure which looks something like this:\n\n\nexample_folder\n├── code\n│   └── analysis.Rmd\n└── data\n    ├── clean_data.csv\n    └── raw_data.csv\n\n\nHowever, in weeks-long or even months-long research projects, if you’re not careful your project will quickly spiral out of control (see the lovely surprise below for a tame example), your R Environment will begin to store many variables, and you’ll begin to pass data objects around between scripts and markdowns in an unstructured way, e.g. you’ll reference a variable created inside ‘wrangling.Rmd’ inside the ‘colab_cleaning.Rmd’, such that colab_cleaning.Rmd becomes unreproducible.\n\n\nA lovely surprise\n\n\n\nexample_folder_complex\n├── code\n│   ├── colab_cleaning.Rmd\n│   ├── edited_functions.R\n│   ├── images\n│   │   └── outline_image.png\n│   ├── initial_analysis.Rmd\n│   ├── quick_functions.R\n│   ├── topic_modelling.Rmd\n│   └── wrangling.Rmd\n└── data\n    ├── clean\n    │   ├── all_data_clean.csv\n    │   ├── all_data_clean_two.csv\n    │   ├── all_data_cleaner.csv\n    │   ├── data_topics_clean.csv\n    │   └── data_topics_newest.csv\n    └── raw\n        ├── sprinklr_export_1.xlsx\n        ├── sprinklr_export_10.xlsx\n        ├── sprinklr_export_11.xlsx\n        ├── sprinklr_export_12.xlsx\n        ├── sprinklr_export_13.xlsx\n        ├── sprinklr_export_14.xlsx\n        ├── sprinklr_export_15.xlsx\n        ├── sprinklr_export_16.xlsx\n        ├── sprinklr_export_17.xlsx\n        ├── sprinklr_export_18.xlsx\n        ├── sprinklr_export_19.xlsx\n        ├── sprinklr_export_2.xlsx\n        ├── sprinklr_export_20.xlsx\n        ├── sprinklr_export_21.xlsx\n        ├── sprinklr_export_22.xlsx\n        ├── sprinklr_export_23.xlsx\n        ├── sprinklr_export_24.xlsx\n        ├── sprinklr_export_25.xlsx\n        ├── sprinklr_export_26.xlsx\n        ├── sprinklr_export_27.xlsx\n        ├── sprinklr_export_28.xlsx\n        ├── sprinklr_export_29.xlsx\n        ├── sprinklr_export_3.xlsx\n        ├── sprinklr_export_30.xlsx\n        ├── sprinklr_export_4.xlsx\n        ├── sprinklr_export_5.xlsx\n        ├── sprinklr_export_6.xlsx\n        ├── sprinklr_export_7.xlsx\n        ├── sprinklr_export_8.xlsx\n        └── sprinklr_export_9.xlsx\n\n\n\n\n10.2.1 Literate Programming\n\n“a script, notebook, or computational document that contains an explanation of the program logic in a natural language (e.g. English or Mandarin), interspersed with snippets of macros and source code, which can be compiled and rerun. You can think of it as an executable paper!”\n\nNotebooks have become the de factor vehicles for Literate Programming and reproducible research. They allow you to couple your code, data, visualisations, interpretations and analysis. You can and should use the knit/render buttons regularly (found in the RStudio IDE) to keep track of whether your code is reproducible or not - follow the error messages to ensure reproducibility.\n\nHave I turned off the restore .RData setting in tools –&gt; global options?\nHave I separated raw data and clean data?\nHave I recorded in code my data cleaning & transformation steps?\nDo my markdowns and notebooks render?\nAm I using relative or absolute filepaths within my scripts & notebooks?\n[ ]\n[ ]",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#on-flow-and-focus",
    "href": "code_best_practices.html#on-flow-and-focus",
    "title": "10  Coding best practices",
    "section": "10.3 On flow and focus",
    "text": "10.3 On flow and focus\nMost of us cannot do our best work on the most difficult challenges for 8 hours per day. In fact, conservative estimates suggest we have 2-3 hours per day, or 4 hours on a good day, where we can work at maximum productivity on challenging tasks. Knowing that about ourselves, we should proactively introduce periods of high and low intensity to our days.\nIn periods of high intensity we’ll be problem solving - inspecting our data, selecting cleaning steps, running small scale experiments on our data: ‘What happens if I…’ and recording and interpreting the results. When the task is at the correct difficulty, you’ll naturally fall into a flow state. Try your best to prevent interruptions during this time. Protect your focus - don’t check your work emails, turn Slack off etc.\nWhilst these high-intensity periods are rewarding and hyper-productive, at the other end there is often a messy notebook or some questionable coding practices. Allocate time each and every day to revisit the code, add supporting comments, write assertions and tests, rename variables to be more descriptive, tidy up unused data artefacts, study your visualisations to understand what the data can really tell you etc. or anything else you can do to let your brain rest, recharge and come back stronger tomorrow. You’ll sometimes feel like you don’t have time to do these things, but it’s quite the opposite - you don’t have time not to do them.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#on-managing-complexity",
    "href": "code_best_practices.html#on-managing-complexity",
    "title": "10  Coding best practices",
    "section": "10.4 On managing complexity",
    "text": "10.4 On managing complexity\n\n“…let’s think of code complexity as how difficult code is to reason about and work with.”\n\nThere are many heuristics for measuring code complexity, the most basic being ‘lines of code’ which is closely linked to ‘vertical complexity’ - the more code we have the longer our scripts and markdowns will be, the harder it is to see all of the relevant code at any one time, the more strain we put on our working memories. A naive strategy for reducing complexity is to reduce lines of code. But if we reduce the number of lines of code by introducing deeply nested function calls, the code becomes more complex not less as the number of lines decreases.\nAs a rough definition, let’s think of code complexity as ‘how difficult code is to reason about and work with.’ A good test of code complexity is how long it takes future you to remember what each line, or chunk, of code is for.\nWe’ll now explore some tools and heuristics for fighting complexity in our code.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#on-navigation",
    "href": "code_best_practices.html#on-navigation",
    "title": "10  Coding best practices",
    "section": "10.5 On navigation",
    "text": "10.5 On navigation\n\n\n\n\n\ngraph LR\n    A[Raw Data] --&gt; B(Cleaning)\n    B --&gt; C(Transformation)\n    C --&gt; D(Visualisation)\n    D --&gt; E(Modelling)\n    E --&gt; F(Communicating Results)\n    E --&gt; B(Cleaning)\n\n\n\n\n\n\nLet’s go out on a limb and say that the data science workflow is never linear, you will always move back and forth between cleanin data, inspecting it, and modelling it. Structuring your projects and notebooks with this in mind will save many headaches.\n\n10.5.1 Readme\nFor each project, add aREADME.md or README.Rmd, here you can outline what and who the project is for and guide people to notebooks, data artefacts, and any important resources. You may find it useful to maintain a to-do list here, or provide high-level findings - it’s really up to you, just keep your audience in mind.\n\n\n10.5.2 Section Titles\n\n\nSection titles help order your thoughts - when done well they let you see the big picture of your document. They will also help your collaborators to navigate and understand your document, and they’ll function as HTML headers in your rendered documents. When in the RStudio IDE the outline tab allows click-to-navigate with your section titles.\n\n\n\n\n\n\nTip\n\n\n\nSet the toc-depth: in your quarto yaml to control how many degrees of nesting are shown in your rendered document’s table of contents.\n\n\n\n\n\n\n\n\nRstudio Outline\n\n\n\n\n\n\n10.5.3 Code chunks\nYou wrote the code in the chunk. So you know what it does, or at least you should. However, when rendering your document (which you should do regularly) it’s handy to have named chunks so that you know precisely which chunk is taking a long time to render, or has a problem. Furthermore, that 8-line pipe inside the chunk might not be as easy to understand at a glance in the future, and it certainly won’t be for your collaborators. It’s much easier to understand what a descriptively named chunk is doing than 8 piped function calls.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#on-comments",
    "href": "code_best_practices.html#on-comments",
    "title": "10  Coding best practices",
    "section": "10.6 On comments",
    "text": "10.6 On comments\nWhen following the literate programming paradigm, coding comments (# comment...) should be included in code chunks with echo = False unless you explicitly want your audience to see the code and the comments - save the markdown text for what your audience needs to see.\nGenerally code comments should be used sparingly, if you find yourself needing a lot of comments it’s a sign the code is too complex, consider re-factoring or abstracting (more on abstractions later).",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#on-repeating-yourself-1---variables",
    "href": "code_best_practices.html#on-repeating-yourself-1---variables",
    "title": "10  Coding best practices",
    "section": "10.7 On repeating yourself #1 - Variables",
    "text": "10.7 On repeating yourself #1 - Variables\nStoring code in multiple places tends to be a liability - if you want to make changes to that piece of code, you have to do it multiple times. More importantly than the time lost making the changes, you need to remember that the code has been duplicated and where all the copies are.\nWithout variables coding would be ‘nasty, brutish and short long.’. It’s difficult to find the Goldilocks zone between ‘more variables than I can possibly name’ and ‘YOLO the project title is typed out 36 times’.\nMagrittr’s pipe operator (%&gt;% or command + shift + m) can save you from having to create too many variables. It would be quite ugly if we had to always code like this:\n\nmpg_horesepower_bar_chart &lt;- ggplot(mtcars, aes(x = mpg, y = hp))\n\nmpg_horesepower_bar_chart  &lt;- mpg_horesepower_bar_chart  + geom_point()\n\nmpg_horesepower_bar_chart  &lt;- mpg_horesepower_bar_chart  + labs(title = \"666 - Peaks & Pits - Xbox Horsepower vs Miles per Gallon\")\n\nmpg_horesepower_bar_chart \n\nInstead of this:\n\nmtcars %&gt;% \n  ggplot(aes(x = mpg, y = hp)) +\n  geom_point() +\n  labs(title = \"666 - Peaks & Pits - Xbox Horsepower vs Miles per Gallon\")\n\nPlace strings you’ll use a lot in variables at the top of your notebook, and then use the paste function, rather than cmd + c, to use the contents of the variable where necessary. This way, when you need to change the title of the project you won’t have to mess around with cmd + f or manually change each title for every plot.\n\nproject_title &lt;- \"666 - Peaks & Pits - Xbox:\"\n\nmtcars %&gt;% \n  ggplot(aes(x = mpg, y = hp)) +\n  geom_point() +\n  labs(title = paste0(project_title, \" Horsepower vs Miles per Gallon\"))\n\n\nGive your variables descriptive names and use your IDE’s tab completion to help you access long names.\n\nLet’s say you’re creating a data frame that you’re not sure you’ll need. Assume you will need it and delete after if not, don’t fall into the trap of naming things poorly\ntmp_df ❌\nscreen_name_counts ✅",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#on-naming",
    "href": "code_best_practices.html#on-naming",
    "title": "10  Coding best practices",
    "section": "10.8 On naming",
    "text": "10.8 On naming\nThe primary objects for which naming is important are variables, functions, code chunks, section titles, and files. Give each of these clear names which describe precisely what they do or why they are there.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#on-repeating-yourself-2---abstractions",
    "href": "code_best_practices.html#on-repeating-yourself-2---abstractions",
    "title": "10  Coding best practices",
    "section": "10.9 On repeating yourself #2 - Abstractions",
    "text": "10.9 On repeating yourself #2 - Abstractions\nDo Not Repeat Yourself, so the adage goes. But some repetition is natural, desirable, and harmless whereas attempts to avoid all repetition can be the opposite. As a rule-of-thumb, if you write the same piece of code three times you should consider creating an abstraction.\nReasonable people disagree on the precise definition of ‘abstraction’ when it comes to coding & programming. For our needs, we’ll think about it as simplifying code by hiding some complexity. A good abstraction helps us to focus only on the important details, a bad abstraction hides important details from us.\nThe main tools for creating abstractions are:\n\nFunctions\nClasses\nModules\nPackages\n\nWe’ll focus on functions and packages.\n\n10.9.1 On functions\nMake them! There are lots of reasons to write your own functions and make your code more readable and re-usable. We can’t hope to cover them all here, but we want to impress their importance. Writing functions will help you think better about your code and understand it on a deeper level, as well as making it easier to read, understand and maintain.\nFor a more comprehensive resource, check in with the R4DS functions section\nAlso see the Tidyverse Design Guide for stellar advice on building functions for Tidyverse functions.\n\n\n10.9.2 On anonymous functions\nFunctions are particularly useful when you want to use iterators like {purrr}’s map family of functions or base R’s apply family. Often these functions are one-time use only so it’s not worth giving them a name or defining them explicitly, in which case you can use anonymous functions.\nAnonymous functions can be called in three main ways:\n\nUsing function() e.g. function(x) x + 2 will add 2 to every input\nUsing the new anonymous function notation: \\x x + 2\nUsing the formula notation e.g. map(list, ~.x + 2)\n\nYou will see a mixture of these, with 3. being used more often in older code, and 2. in more recent code.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#on-packages",
    "href": "code_best_practices.html#on-packages",
    "title": "10  Coding best practices",
    "section": "10.10 On packages",
    "text": "10.10 On packages\nDepending on how many functions you’ve created, how likely you are to repeat the analysis, and how generalisable the elements of your code are, it may be time to create a package.\nAt first building a package is likely to seem overwhelming and something that ‘other people do’. However, in reality the time it takes to create a package reduces rapidly the more you create them. And the benefits for sharing your code with others are considerable. Eventually you’ll be able to spin up a new package for personal use in a matter of minutes, over time it will become clear which packages should be developed, left behind, or merged into an existing SAMY package.\nVisit the Package Development document for practical tips and guidelines for developing R packages\nsee also: Package Resources",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#on-namespaces-and-function-conflicts",
    "href": "code_best_practices.html#on-namespaces-and-function-conflicts",
    "title": "10  Coding best practices",
    "section": "10.11 On namespaces and function conflicts",
    "text": "10.11 On namespaces and function conflicts\nR manages functions via namespaces. Depending on the order that you import your packages, you may find a function doesn’t behave as expected. For example, the {stats} package has a filter() function, but so does {dplyr}. By default R will use the most recently-imported package’s namespace to avoid any conflicts, so filter() will now refer to {dplyr}’s implementation.\nIf you’re experiencing weirdness with a function, you may want to restart your R session, change the order of your imports to prevent the same weirdness occurring again. However, a more straightforward approach is to use the package:: notation to explicitly refer to the function you intended to use e.g. dplyr::filter() will avoid any potential conflicts and confusion.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#on-version-control",
    "href": "code_best_practices.html#on-version-control",
    "title": "10  Coding best practices",
    "section": "10.12 On version control",
    "text": "10.12 On version control\nBy default your projects should be stored on Google Drive inside the “data_science_project_work” folder, in the event of disaster (or minor inconvenience) this means your code and data artefacts should be backed up. However, it’s still advisable to use a version control system like git - using branches to explore different avenues, or re-factor your code, can be a real headache preventer and efficiency gain.\nAim to commit your code multiple times per day, push to a remote branch (not necessarily main or master) once a day and merge + pull request when a large chunk of work has been finished. Keep your work code and projects in a private repository, add .Rhistory to .gitignore and make sure API keys are stored securely, i.e. not in scripts and notebooks.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#on-managing-dependencies",
    "href": "code_best_practices.html#on-managing-dependencies",
    "title": "10  Coding best practices",
    "section": "10.13 On Managing Dependencies",
    "text": "10.13 On Managing Dependencies\nApplying the Anna Karenina principle to virtual environments:\n\n“All happy virtual environments are alike; each unhappy virtual environment is unhappy in its own way”\n\n\n10.13.1 R\nThe R ecosystem - led by CRAN and posit (formerly RStudio) - does a great job in managing package-level dependencies. It’s rare to end up in dependency hell R. However, there is still scope for ‘works on my machine’ when working with collaborators who have different versions of a package, e.g. person 1 upgrades their {dplyr} version before person 2, and now that .by argument person 1 has used is breaking code on person 2’s machine.\nTo avoid this, we advise using something like the {renv} package to manage package versions.\n\n\n\n\n\n\nOn using renv collaboratively\n\n\n\n\n\nrenv helps us keep track of package & R versions, which makes deployment 10x easier than without it. However, it can get tricky if we’re using renv in different ways.\nStart a renv project off withrenv::init(), this will essentially remove all of your packages. You could choose to take your current packages with you, but it’s not advised. These packages are linked to your RStudio project when using RStudio, which means other RStudio projects will have your current packages if they’re not themselves using renv.\nrenv::init() creates a renv lockfile, you’ll need to keep this up to date as you work through the project - especially important if collaborating with other people on a project that uses renv. Once installed inside your local project, you can add single packages to the lockfile by renv::record(\"package_name\"). This is preferable to adding a bunch of packages at a time with renv::snapshot() particularly when collaborating. Generally it will be better to give one person control of the lockfile and to communicate about adding packages as and when.\nIf you’re working in a Quarto Doc or an RMarkdownfile to develop things that don’t need to pushed to the repo, you can create a .renvignore file like .Rbuildignore, .gitignore etc. and add the folder where the markdowns sit to make sure renv doesn’t try to sync itself with the packages you’re experimenting with.\nAt any time you can check your environment is still synced with renv::status(), if your project is out of sync, you may want to renv::clean(), if you’ve got a bunch of packages that are like this:\nThe following package(s) are in an inconsistent state:\npackage       installed recorded used\nbackports     y         y        n\nblob          y         y        n\nbroom         y         y        n\ncallr         y         y        n\ncellranger    y         y        n\nThen you’ll need to renv::clean(actions = \"unused.packages\"), which should get you in working order. There’s a lot more to {renv} when collaborating but these steps will do a lot to keep your environment in sync and allow collaborators to use your code.\n\n\n\n\n\n10.13.2 Python\n\nUnlike R, it’s pretty easy to get into deep, deep trouble when working with Python environments. We advise using miniconda, keeping your base environment completely free of packages, and creating virtual environments for projects or large, but commonly-used and important packages like torch.\n\n\n\n\n\n\nTip\n\n\n\nJust remember to activate the correct environment every time you need to install a package!",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#on-llm-generated-code",
    "href": "code_best_practices.html#on-llm-generated-code",
    "title": "10  Coding best practices",
    "section": "10.14 On LLM-generated code",
    "text": "10.14 On LLM-generated code\nGitHub Copilot, ChatGPT, Claude and other LLM-based code generators can be extremely useful, but they are a double-edged sword and should be used responsibly. If you find yourself relying on code you don’t understand, or couldn’t re-build yourself, you’re going to run in to trouble somewhere down the line. You have the time and space to learn things deeply here, so do read the docs, do reference textbooks, and do ask for help internally before relying on LLM-generated code which often looks right but is outdated or subtly incorrect/buggy.\n\n\n\n\n\n\nTip\n\n\n\nYou’re here because you can problem solve and pick up new skills when you need them - don’t be afraid to spend extra time understanding a concept or a library.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#great-coding-resources",
    "href": "code_best_practices.html#great-coding-resources",
    "title": "10  Coding best practices",
    "section": "10.15 Great Coding Resources",
    "text": "10.15 Great Coding Resources\n\nGoogle SWE Book\nHands on programming with R\nReproducible Analysis, Jenny Bryan",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Coding best practices</span>"
    ]
  },
  {
    "objectID": "code_best_practices.html#exercises",
    "href": "code_best_practices.html#exercises",
    "title": "10  Coding best practices",
    "section": "10.16 Exercises",
    "text": "10.16 Exercises\n\nIn your own words, summarise what makes an analysis reproducible.\n\n\nWrite a line in favour and against the claim ‘Code is an asset not a liability.’\n\n\nSet up a private github repo on a project inside data_science_project_work/internal_projects and create a new branch then commit, push and pull request a change.\n\n\nAdd your own best practices to this document!",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Coding best practices</span>"
    ]
  }
]