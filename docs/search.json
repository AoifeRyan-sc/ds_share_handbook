[
  {
    "objectID": "calling_apis.html",
    "href": "calling_apis.html",
    "title": "12  Calling APIs",
    "section": "",
    "text": "12.1 Why Do We Use APIs?\nAs a team, our main use case for APIs is the OpenAI API, which grants us access to the advanced AI models developed by OpenAI, including the GPT (text), DALL-E (image generation), and Whisper (speech-to-text) models. One of the key advantages of using an API instead of downloading and running these models locally (or utilising open-source models) is that it allows us to leverage the computational power and optimization of the models without needing expensive hardware or vast computational resources.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Calling APIs</span>"
    ]
  },
  {
    "objectID": "calling_apis.html#openai-api-overview",
    "href": "calling_apis.html#openai-api-overview",
    "title": "12  Calling APIs",
    "section": "12.2 OpenAI API Overview",
    "text": "12.2 OpenAI API Overview\nOpenAI’s API is a REST (Representational State Transfer) API. Simply put, this allows a client (such as our program) to request data or perform actions on a server (which hosts the AI models), where it retrieves or manipulates resources (e.g., model outputs such as generated text).\n\n12.2.1 How the API Works\nOpenAI’s API works on the standard HTTP protocol, which structures communication between the client and server. In this system:\n\nEndpoints are specific paths on the server where the API can be accessed. For example, /v1/chat/completions is an endpoint that allows us to send prompts to a GPT model and receive completions.\nRequests are the actions taken by our application. We send requests with specific inputs (like text prompts), and the API processes them.\nResponses are the API’s outputs, such as text from a GPT model, an image from DALL-E, or speech-to-text conversions from Whisper.\n\n\n\n12.2.2 Understanding Tokens and Model Usage\nAPIs like OpenAI’s typically have costs associated with their usage, and this is often measured in tokens. When you input text or data into an API, it is broken down into tokens, which are individual units of language (like parts of words or characters).\n\nInput Tokens: These are the tokens sent to the API (e.g., your prompt to GPT). Every word, punctuation mark, or whitespace counts toward your input tokens.\nOutput Tokens: These are the tokens returned from the API as a response (e.g., the AI’s reply). The longer and more complex the output, the more tokens are consumed.\n\nManaging tokens is crucial because they directly impact the cost of API usage. Different models have different costs per token, with more advanced models being more expensive but often providing better results. For example, as of writing this document (October 2024), the pricing for gpt-4o-mini is $0.150/1M input tokens and $0.500/1M output tokens compared to $5.00/1M input tokens and $15.00/1M output tokens for gpt-4o. Or in other words, gpt-4o-mini is ~30x cheaper than gpt-4o! Check out the model pricing information to see the latest costs, and the model pages to see the differences in model capabilities (i.e. context windows, maximum output tokens, training data).\n\n\n\n\n\n\nTokens to Words\n\n\n\n\n\n\nA token is typically about 4 characters of English text.\n100 tokens are roughly equivalent to 75 words.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Calling APIs</span>"
    ]
  },
  {
    "objectID": "calling_apis.html#practical-use-of-openais-api",
    "href": "calling_apis.html#practical-use-of-openais-api",
    "title": "12  Calling APIs",
    "section": "12.3 Practical Use of OpenAI’s API",
    "text": "12.3 Practical Use of OpenAI’s API\nBasically, we use the API in the same way as we would use other public APIs - sign up for an account, obtain an API key, and then use the key to make API calls to specific models using HTTP requests.\n\n12.3.1 Step One - Obtain API Key and Authentication\nTo start using OpenAI’s API, you’ll need an API key for authentication. Follow these steps:\n\nGo to platform.openai.com and create an account using your SHARE email address.\nMike will add you to the “SHARE organization” within the platform, allowing you to access the set aside usage credits we have as a company.\nThen make your way to the api-keys section of the platform and click the green Create new secret key in the top corner.\n\n\n\n\nCreate new secret key by clicking the button in the top right hand corner\n\n\n\nRename the key to something useful, such as the name and number of the project that they key will be used for, and keep the OpenAI project as “Default project” and Permissions as “All”.\nYou will then be provided with the opportunity to copy the provided API key, this is the one chance you will get to obtain it- after you click off this pop up you won’t be able to view the full API key again and you’ll need to request a new one. Because of this, make sure you copy the key and add it to this private Google Sheet where the DS team keeps the API Keys. Remember that using the API costs money, so if this key is used by others we risk someone using up all of our API credits! Please see below for some more best practices relating to API key security.\n\n\n\n12.3.2 Step Two - Managing API Keys Securely\nAs outlined above, when working with APIs it’s essential to manage our API keys securely. An API key grants access to services, and if exposed, others could misuse it, leading to security breaches, unauthorised usage, or unexpected costs. Here are some key principles to follow:\n\nNever Hard-Code API Keys Avoid storing API keys directly in your code as hard-coded variables. This exposes them to anyone with access to your codebase.\nUse Environment Variables Store API keys in environment variables to keep them separate from the code. This ensures sensitive data isn’t exposed, and it’s easier to manage keys across different environments if required (development, production, etc.).\nVersion Control Precautions Make sure to add environment files that contain sensitive information (like .env or .Renviron) to .gitignore so they don’t get uploaded to version control systems like GitHub. Exposing API keys in public repositories is a common mistake, and it can be a serious security risk.\n\n\n12.3.2.0.1 Example implementations\nRStudio\n\nAdd API Key to .Renviron\n\nUse the usethis package to edit the .Renviron file where environment variables are stored. Add the API key like this:\n\nusethis::edit_r_environ(scope = \"project\")\n\nThis will open the .Renviron file in your editor. Note that scope = \"project\" scope means that the .Renviron file will be created in your specific R project folder. This means the environment variables (like your API key) will only be available when you are working inside that project. It’s a good way to keep project-specific configuration separate from other projects.\nThen add the following line to store your API key (replace your-api-key-here with the actually API key)\n\n# Write this within the .Renviron file and save it\nOPENAI_API_KEY=your-api-key-here\n\n\nAccess the API Key in your R Script\n\nYou can access the API key in your R scripts using Sys.getenv()\n\napi_key &lt;- Sys.getenv(\"OPENAI_API_KEY\")\n\nor if you need to call the API key in a function (such as BERTopicR) it could be\n\nrepresentation_openai &lt;- bt_representation_openai(fitted_model,\n                                                  documents,\n                                                  openai_model = \"gpt-4o-mini\",\n                                                  nr_repr_docs = 10,\n                                                  chat = TRUE,\n                                                  api_key = Sys.getenv(\"OPENAI_API_KEY\"))\n\n\nAdd .Renviron to .gitignore\n\nObviously this is only relevant if you are deploying a repo/project to GitHub, but we can make sure to exclude the .Renviron file to our .gitignore file\n\n# Exclude .Renviron file\n.Renviron\n\nPython\n\nCreate a .env file\n\nIn the root directory of your project, create a .env file. The best way to do this is using command line tools (touch and nano)\nWithin the terminal create an empty .env file by running\n\ntouch .env\n\nand then edit it by running\n\nnano .env\n\nand finally within the nano editor, type the following to add your API key (replace your-api-key-here with the actually API key)\n\nOPENAI_API_KEY=your-api-key-here\n\n\nUse the python-dotenv library\n\nInstall python-dotenv by running\n\npip install python-dotenv\n\n\nAccess the API Key in your script\n\nIn your Python script, load the .env file and access the API key\n\nfrom dotenv import load_dotenv\nimport os\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Access the API key\napi_key = os.getenv(\"OPENAI_API_KEY\")\n\n\nAdd .env to .gitignore\n\nSimilar to the RStudio implementation above, add .env to your .gitignore\n\n# Exclude .env file\n.env\n\n\n\n\n12.3.3 Step Three - Making Requests to the API\nTo actually make requests to the OpenAI API we use python, and specifically the official OpenAI SDK. You can install it to your python environment simply via pip by running\n\npip install openai\n\nThe documentation on-line surrounding calling the OpenAI API is extremely extensive and generally good, however the API and underlying models do get updated quite often and this can cause code to become redundant or not act as one may expect. This can be particularly unwelcome when you run a previously working script to ping the API, get charged, but don’t receive an output that is useful.\nThe simple way to call the API and obtain a ‘human-like response’ to a prompt is with this code adapted from the OpenAI API tutorial:\n\nfrom openai import OpenAI\nclient = OpenAI(api_key = OPENAI_API_KEY)\n\ncompletion = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Write a short poem about RStudio.\"}\n    ]\n)\n\nprint(completion.choices[0].message)\n\nDon’t worry about what everything means, we’ll explain this in a bit more detail below. But firstly, one thing to realise is that this code above is effectively the same as going onto the ChatGPT website and typing into the input box “You are a helpful assistant. Write a short poem about RStudio.” for the model gpt-4o-mini. So effectively this code calls the API once, with an input and receives an output from the model.\n\n12.3.3.1 Chat Completions\nTo use one of the text models, we need to send a request to the Chat Completions API containing the inputs and our API key, and receive a response containing the model’s output.\nThe API accepts inputs by the parameter messages, which is an array of message objects. Each message object has a role, either system, user, or assistant, and content.\n\nThe system message is optional and can be used to set the behaviour of the assistant\nThe user messages provide requests or comments for the assistant to respond to\nAssistant messages store previous assistant responses, but can also be written by us to give examples of desired behaviour (however note we can also provide examples within the user message- which is what we tend to do in our workflows)\n\nFor example:\n\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n  ]\n)\n\nWhilst this chat format is designed to work well with multi-turn conversations, in reality we use it for single-turn tasks without a full conversation. So we would normally have something more like:\n\nfrom openai import OpenAI\nclient = OpenAI(api_key = OPENAI_API_KEY)\n\ncompletion = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant who specialised in sentiment analysis\"},\n        {\"role\": \"user\", \"content\": \"What is the sentiment of the following text: 'I love reading this Handbook'\"}\n    ]\n)\n\nprint(completion.choices[0].message)\n\nThe response (defined as completion in the code above) of the Chat Completions API looks like the following:\n\n{\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"The sentiment of the text 'I love reading this Handbook' is positive. The use of the word 'love' indicates a strong positive emotion towards the Handbook.\",\n        \"role\": \"assistant\"\n      },\n      \"logprobs\": null\n    }\n  ],\n  \"created\": 1677664795,\n  \"id\": \"chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW\",\n  \"model\": \"gpt-4o-mini\",\n  \"object\": \"chat.completion\",\n  \"usage\": {\n    \"completion_tokens\": 26,\n    \"prompt_tokens\": 13,\n    \"total_tokens\": 39,\n    \"completion_tokens_details\": {\n      \"reasoning_tokens\": 0\n    }\n  }\n}\n\nWe can see there is a lot of information here, such as the model used and the number of input tokens. You will notice the response is a dictionary and made up of key-value pairs to help organise the relevant information. However, we are mostly focussed on the models output (that is, the assistants reply), which we can extract by running:\n\nmessage = completion.choices[0].message.content\n\n\n\n\n12.3.4 Best Practices for OpenAI’s API\n\nUse the Playground- The Playground is a tool from OpenAI that you can use to learn how to construct prompts, so that you can get comfortable using querying the model and learning how the API works. Note however that using the Playground incurs costs. While it’s unlikely you will rack up a large bill be analysing large corpora within the playground, it is still important to be mindful of usage.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Calling APIs</span>"
    ]
  },
  {
    "objectID": "calling_apis.html#to-do",
    "href": "calling_apis.html#to-do",
    "title": "12  Calling APIs",
    "section": "12.4 To Do",
    "text": "12.4 To Do\n[] Best practices\n[] Structured Outputs\n[] Cost efficiency/more detailed paramters (max_tokens etc)\n[] Batch processing\n\n\n\nCreate new secret key by clicking the button in the top right hand corner",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Calling APIs</span>"
    ]
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "11  Data Cleaning",
    "section": "",
    "text": "11.1 Dataset-level cleaning\nGoal: Ensure the dataset as a whole is relevant and of high quality\nThe main steps that we take for this level of cleaning is spam removal, uninformative content removal and deduplication",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data_cleaning.html#dataset-level-cleaning",
    "href": "data_cleaning.html#dataset-level-cleaning",
    "title": "11  Data Cleaning",
    "section": "",
    "text": "11.1.1 Spam Removal\nWe use the term “spam” quite loosely in our data pre-processing workflows. Whilst the strict definition of “spam” could be something like “unsolicited, repetitive, unwanted content”, we can think of it more broadly any post that displays irregular posting patterns or is not going to provide analytical value to our research project.\n\n11.1.1.1 Hashtag filtering\nThere are multiple ways we can identify spam to remove it. The simplest is perhaps something like hashtag spamming, where an excessive number of hashtags, often unrelated to the content of the post, can be indicative of spam.\nWe can identify posts like this by counting the number of hashtags, and then filtering out posts that reach a certain (subjective) threshold.\n\ncleaned_data &lt;- data %&gt;% \n  mutate(extracted_hashtags = str_extract_all(message_column, \"#\\\\S+\"),\n         number_of_hashtags = lengths(extracted_hashtags)) %&gt;% \n  filter(number_of_hashtags &lt; 5)\n\nIn the example above we have set the threshold to be 5 (so any post that has 5 or more hashtags will be removed), however whilst this is a valid starting point, it is highly recommend to treat each dataset uniquely in determining which threshold to use.\n\n\n11.1.1.2 Spam-grams\nOften-times spam can be identified by repetitive posting of the same post, or very similar posts, over a short period of time.\nWe can identify these posts by breaking down posts into n-grams, and counting up the number of posts that contain each n-gram. For example, we might find lots of posts with the 6-gram “Click this link for amazing deals”, which we would want to be removed.\nTo do this, we can unnest our text data into n-grams (where we decide what value of n we want), count the number of times each n-gram appears in the data, and filter out any post that contains an n-gram above this filtering threshold.\nThankfully, we have a function within the LimpiaR package called limpiar_spam_grams() which aids us with this task massively. With this function, we can specify the value of n we want and the minimum number of times an n-gram should occur to be removed. We are then able to inspect the different n-grams that are removed by the function (and their corresponding post) optionally changing the function inputs if we need to be more strict or conservative with our spam removal.\n\nspam_grams &lt;- data %&gt;% \n  limpiar_spam_grams(text_var = message_column,\n                     n_gram = 6,\n                     min_freq = 6)\n\n# see remove spam_grams\nspam_grams %&gt;% \n  pluck(\"spam_grams\")\n\n# see deleted posts\nspam_grams %&gt;% \n  pluck(\"deleted\")\n\n# save 'clean' posts\nclean_data &lt;- spam_grams %&gt;% \n  pluck(\"data\")\n\n\n\n11.1.1.3 Filter by post length\nDepending on the specific research question or analysis we will be performing, not all posts are equal in their analytical potential. For example, if we are investigating what specific features contribute to the emotional association of a product with a specific audience, a short post like “I love product” (three words) won’t provide the level of detail required to answer the question.\nWhile there is no strict rule for overcoming this, we can use a simple heuristic for post length to determine the minimum size a post needs to be before it is considered informative. For instance, a post like “I love product, the features x and y excite me so much” (12 words) is much more informative than the previous example. We might then decide that any post containing fewer than 10 words (or perhaps 25 characters) can be removed from downstream analysis.\nOn the other end of the spectrum, exceedingly long posts can also be problematic. These long posts might contain a lot of irrelevant information, which could dilute our ability to extract the core information we need. Additionally, long posts might be too lengthy for certain pipelines. Many embedding models, for example, have a maximum token length and will truncate posts that are longer than this, meaning we could lose valuable information if it appears at the end of the post. Also, from a practical perspective, longer posts take more time to analyse and require more cognitive effort to read, especially if we need to manually identify useful content (e.g. find suitable verbatims).\n\n# Remove posts with fewer than 10 words\ncleaned_data &lt;- data %&gt;% \n  filter(str_count(message_column, \"\\\\w+\") &gt;= 10)\n\n# Remove posts with fewer than 25 characters and more than 2500 characters\ncleaned_data &lt;- data %&gt;% \n  filter(str_length(message_column) &gt;= 25 & str_length(message_column) &lt;= 2500)\n\n\n\n\n11.1.2 Deduplication\nWhile removing spam often addresses repeated content, it’s also important to handle cases of exact duplicates within our dataset. Deduplication focuses on eliminating instances where entire data points, including all their attributes, are repeated.\nA duplicated data point will not only have the same message_column content but also identical values in every other column (e.g., universal_message_id, created_time, permalink). This is different from spam posts, which may repeat the same message but will differ in attributes like universal_message_id and created_time.\nAlthough the limpiar_spam_grams() function can help identify spam through frequent n-grams, it might not catch these exact duplicates if they occur infrequently. Therefore, it is essential to use a deduplication step to ensure we are not analysing redundant data.\nTo remove duplicates, we can use the distinct() function from the dplyr package, ensuring that we retain only unique values of universal_message_id. This step guarantees that each post is represented only once in our dataset.\n\ndata_no_duplicates &lt;- data %&gt;% \n  distinct(universal_message_id, .keep_all = TRUE)",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data_cleaning.html#document-level-cleaning",
    "href": "data_cleaning.html#document-level-cleaning",
    "title": "11  Data Cleaning",
    "section": "11.2 Document-level cleaning",
    "text": "11.2 Document-level cleaning\nGoal: Prepare each individual document (post) for text analysis.\nAt a document-level (or individual post level), the steps that we take are more small scale. The necessity to perform each cleaning step will depend on the downstream analysis being performed, but in general the different steps that we can undertake are:\n\n11.2.1 Remove punctuation\nOften times we will want punctuation to be removed before performing an analysis because they tend to not be useful for text analysis. This is particularly the case with more ‘traditional’ text analytics, where an algorithm will assign punctuation marks a unique numeric identify just like a word. By removing punctuation we create a cleaner dataset by reducing noise.\n\n\n\n\n\n\nWarning on punctuation\n\n\n\n\n\nFor more complex models, such as those that utilise word or sentence embeddings, we often keep punctuation in. This is because punctuation is key to understanding a sentences context (which is what sentence embeddings can do).\nFor example, there is a big difference between the sentences “Let’s eat, Grandpa” and “Let’s eat Grandpa”, which is lost if we remove punctuation.\n\n\n\n\n\n11.2.2 Remove stopwords\nStopwords are extremely common words such as “and,” “the,” and “is” that often do not carry significant meaning. In text analysis, these words are typically filtered out to improve the efficiency of text analytical models by reducing the volume of non-essential words.\nRemoving stopwords is particularly useful in our projects for when we are visualising words, such as a bigram network or a WLO plot, as it is more effective if precious informative space on the plots is not occupied by these uninformative terms.\n\n\n\n\n\n\nWarning on stopword removal\n\n\n\n\n\nSimilarly to the removal of punctuation, for more complex models (those that utilise word or sentence embeddings) we often keep stopwords in. This is because these stopwords can be key to understanding a sentences context (which is what sentence embeddings can do).\nFor example, imagine if we removed the stopword “not” from the sentence “I have not eaten pizza”- it would become “I have eaten pizza” and the whole context of the sentence would be different.\nAnother time to be aware of stopwords is if a key term related to a project is itself a stopword. For example, the stopwords list SMART treats the term “one” as a stopword. If we were studying different Xbox products, then the console “Xbox One” would end up being “Xbox” and we would lose all insight referring to that specific model. For this reason it is always worth double checking which stopwords get removed and whether it is actually suitable.\n\n\n\n\n\n11.2.3 Lowercase text\nConverting all text to lowercase standardises the text data, making it uniform. This helps in treating words like “Data” and “data” as the same word, and is especially useful when an analysis requires an understanding of the frequency of a term (we rarely want to count “Data” and “data” as two different things) such as bigram networks.\n\n\n11.2.4 Remove mentions\nMentions (e.g., @username) are specific to social media platforms and often do not carry significant meaning for text analysis, and in fact may be confuse downstream analyses. For example, if there was a username called @I_love_chocolate, upon punctuation remove this might end up confusing a sentiment algorithm. Removing mentions therefore helps in focusing on the actual content of the text.\n\n\n\n\n\n\nRetaining mentions, sometimes\n\n\n\n\n\nWe often perform analyses that involve network analyses. For these, we need to have information of usernames because they appear when users are either mentioned or retweeted. In this case we do not want to remove the @username completely, but rather we can store this information elsewhere in the dataframe.\nHowever, broadly speaking if the goal is to analyse the content/context of a paste, removing mentions is very much necessary.\n\n\n\n\n\n11.2.5 Remove URLs\nURLs in posts often point to external content and generally do not provide meaningful information for text analysis. Removing URLs helps to clean the text by eliminating these irrelevant elements.\n\n\n11.2.6 Remove emojis/special characters\nEmojis and special characters can add noise to the text data. While they can be useful for certain analyses (like emoji-specific sentiment analysis - though we rarely do this), they are often removed to simplify text and focus on word-based analysis.\n\n\n11.2.7 Stemming/Lemmatization\nStemming and lemmatization are both techniques used to reduce words to their base or root form and act as a text normalisation technique.\nStemming trims word endings to their most basic form, for example changing “clouds” to “cloud” or “trees” to “tree”. However, sometimes stemming reduces words to a form that doesn’t make total sense such as “little” to “littl” or “histories” to “histori”.\nLemmatization considers the context and grammatical role when normalising words, producing dictionary definition version of words. For example “histories” would become “history”, and “caring” would become “car” (whereas for stemming it would become “car”).\nWe tend to use lemmatization over stemming- despite it being a bit slower due to a more complex model, the benefit of lemmitization outweighs this. Similar to lowercasing the text, lemmitization is useful when we need to normalise text where having distinct terms like “change”, “changing”, “changes”, and “changed” isn’t necessary and just “change” is suitable.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data_cleaning.html#conclusion",
    "href": "data_cleaning.html#conclusion",
    "title": "11  Data Cleaning",
    "section": "11.3 Conclusion",
    "text": "11.3 Conclusion\nDespite all of these different techniques, it is important to remember these are not mutually exclusive, and do not always need to be performed. It may very well be the case where a specific project actually required us to mine through the URLs in social posts to see where users a linking too, or perhaps keeping text as all-caps is important for how a specific brand or product is mentioned online. Whilst we can streamline the cleaning steps by using the ParseR function above, it is always worth spending time considering the best cleaning steps for each specific part of a project. It is much better spending more time at the beginning of the project getting this right, than realising that the downstream analysis are built on dodgy foundations and the data cleaning step needs to happen again later in the project, rendering intermediate work redundant.",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "13  Resources",
    "section": "",
    "text": "13.1 Data Visualisation",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#data-visualisation",
    "href": "resources.html#data-visualisation",
    "title": "13  Resources",
    "section": "",
    "text": "Capture Cookbook\nR Graph Gallery",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#literate-programming",
    "href": "resources.html#literate-programming",
    "title": "13  Resources",
    "section": "13.2 Literate Programming",
    "text": "13.2 Literate Programming\n\nQuarto Guide\nQuarto Markdown Basics\nRStudio R Markdown guide\nVisual R Markdown",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#package-development",
    "href": "resources.html#package-development",
    "title": "13  Resources",
    "section": "13.3 Package Development",
    "text": "13.3 Package Development\n\nR Packages Book\npkgdown\nroxygen2\nusethis\ndevtools\ntestthat\nTidyverse Design Guide\nHappy Git with R",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#youtube-channels",
    "href": "resources.html#youtube-channels",
    "title": "13  Resources",
    "section": "13.4 YouTube Channels",
    "text": "13.4 YouTube Channels\nThese channels will help you see the power and flexibility of data analysis & science in R:\n\nJulia Silge\nDavid Robinson",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#shiny",
    "href": "resources.html#shiny",
    "title": "13  Resources",
    "section": "13.5 Shiny",
    "text": "13.5 Shiny\n\nMastering Shiny\nEngineering Enterprise-grade Shiny Applications with Golem\nOutstanding Shiny Interfaces\nModularising Shiny Apps, YT\nPresenting Golem, YT\nStyling Shiny, Joe Chen, YT",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#text-analytics",
    "href": "resources.html#text-analytics",
    "title": "13  Resources",
    "section": "13.6 Text Analytics",
    "text": "13.6 Text Analytics\n\nText Mining with R (Tidytext)\nSupervised Machine Learning for Text Analysis in R\nSpeech & Language Processing\nNatural Language Processing with Transformers - we have a physical copy floating around the office",
    "crumbs": [
      "Tips and Tricks",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Resources</span>"
    ]
  }
]