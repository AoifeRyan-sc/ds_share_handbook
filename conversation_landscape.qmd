# Conversation Landscape

The 'Conversation Landscape' method has proven to be an effective tool for querying, auditing, and analyzing both broad concepts and finely grained topics across social conversations on all major platforms, as well as web pages and forums.

### Project Background

Working with semi-structured or unstructured high-dimensional data, such as text (and in this case, social media posts), poses significant challenges in measuring or quantifying the language used to describe any specific phenomena. One common approach to quantifying language is topic modeling, where a corpus (or collection of documents and in this case, social media posts) is processed and represented in a simplified format. This often involves displaying top terms, verbatims, or threads highlighting any nuances or differences within the data. Traditional topic modeling or text analysis methods, such as Latent Dirichlet Allocation (LDA), operate on the probability or likelihood of terms or n-grams belonging to a set number of topics.

The Conversation Landscape workflow offers a slightly different solution and one that partitions our high-dimensional data without requiring data frames or burdening the user with sifting through rows of data to segment (or sense-check already segmented) data in order to understand or recognize differences across texts, which we will typically later define as topics. This is achieved through sentence transforming, where documents are converted from words to numerical values, which are often referred to as 'embeddings'. These values are calculated based on the content's semantic and syntactic properties. The transformed values are then processed again using dimension reduction techniques, making the data more suitable for visualization. Typically, this involves reducing to two dimensions, though three dimensions may be used to introduce another layer of abstraction between our data points.

Note: This document will delve deeper into the core concepts of sentence transforming and dimension reduction, along with the different methods used to cluster or group topics once the overall landscape is mapped out, referring back to an illustrated real-world business use case of these technique.

#### Final output of project

An ideal output should always showcase the positioning of our reduced data points onto the semantic space, along with any topic or subtopic explanations alongside, using color coding where appropriate. While we sometimes provide raw counts of documents per topic/subtopic, we always include the percentage of topic distribution across our data, occasionally referred to as Share of Voice (SOV).

![Screenshot Taken from the Final Output of an AI Landscape Microsoft Project - Q2 FY24](/img/ai_landscape_output_example.png){fig-alt="A screenshot of a powerpoint slide showcasing a Topic/Subtopic Breakdown of the Artificial Intelligence Conversation"}

## How to get there

As promised, we will provide some more context as well as the appropriate information surrounding the required steps taken, so that a reader may replicate and implement the methods mentioned throughout so far, providing an efficient analysis tool for any set of documents or corpus regardless of domain specifics. While the output provided above showcases a simplified means for visualizing complex and multifaceted noisy data such as the 'Artificial Intelligence' conversation on social, there are a number of steps that one must take and be mindful of throughout in order to create the best fit model and an optimal output for a typical Conversation Landscape project.

The broad steps would include, and as one might find across many projects within the realms of Natural Language Processing (NLP):

-  Initial Exploratory Data Analysis: Checking that the data is relevant and fit to answer the brief.
222   

- Cleaning and Processing: Removal of spam, unhelpful or irrelevant data, and pre-processing of text variable for embedding.

-  Transforming/Embedding: Turning our words into numbers which will later be processed again before being visualized.

-  Dimension Reduction: Reducing our representational values of documents to a manageable state in order to visualize.

-  Topic Modeling/Clustering: Scientifically modeling and defining our data into a more digestable format.


#### Exploratory Data Analysis (EDA):

Whether the user is responsible for data querying/collection or not, the first steps in our workflow should always involve some high-level checks before we proceed with any of the following steps in order to save time downstream and give us confidence to carry over into the data cleaning and processing steps.

First, one should always check things like the existing variables and clean or rename any where necessary. This step requires a little forward thinking as to what columns are necessary to complete each stage of the project. Once happy with our variables, we can then check for things such as missing dates, or if there are any abnormal distributions across columns like Social Platform that might skew any findings or help to explain or justify the resulting topic model at the end of the workflow. We can then do some bespoke or project specific checks like searching for likely-to-find strings or terms within our text variable to ensure the data is relevant and query had captured the phenomena we are aiming to model.

#### Data Cleaning/Processing:

Again, as we may not always be responsible for data collection we can expect that our data may contain unhelpful or even problematic information which is often the result of data being unwillingly bought in by the query. Our job at this stage is to minimize the amount of spam or unhelpful data existing in our corpus to ensure our findings are accurate and appropriate to represent the data that we are modelling.

Optimal procedures for spam detection and removal are covered in more detail *here(will include link when this section is complete)*. However, there are steps one absolutely must take to ensure that the text variable provided to the sentence transformer model is clean and concise so that an accurate and consistent embedding process can take place upon our documents. This includes the removal of:

-   Hashtags #Ô∏è‚É£
-   User/Account Mentions üí¨
-   URLs or Links üåê
-   Emojis üêô
-   Non-English Characters üâê

Often, we might also choose to remove punctuation and/or digits, however in our provided example, we have not done so. There are also things to beware of such as documents beginning with numbers that can influence the later processes so unless we deem them necessary, we should remove these where possible to ensure no inappropriate grouping of documents takes place based on these minor similarities. This is because when topic modelling, we aim to capture the pure essence of clusters based on the contents and their semantic meaning, as apposed to any similarities across the chosen format of documents.

#### Sentence Transforming/Embedding:

Once we are happy with the cleanliness and relevance of our data, including the processing steps taken with our chosen text variable, we can begin embedding our documents so that we have a numerical representation for each that can later be reduced and visualized. Typically, and in this case we have used ready pre-trained sentence transformer models that are hosted on Hugging Face, such as `all-mpnet-base-v2` which we had decided to use for the Microsoft AI project. This is because of great performance scores for how lightweight the model was at the time of the project, however with models such as these being open-source, community-lead contributions are made to further train and improve model performance which means that performance metrics are always increasing, so one may wish to consult the [Hugging Face leaderboard](https://huggingface.co/spaces/mteb/leaderboard), or simply do some desk research before settling on an ideal model appropriate for their own specific use case.

While the previous steps might have been taken using R and Rstudio and making use of SHARE's suite of data cleaning, processing and parsing functionality, the embedding process will need to be completed using Google Colab. This is to take advantage of their premium GPUs and high RAM option, as embedding documents can require large amounts of compute, so much so that most fairly competent machines with standard tech specs will struggle. It is also worth noting that an embedding output may depend on the specific GPU being utilized as well as the version of Python that Colab is currently running, its good practice to make note of both of these (and you may thank yourself at a later stage for doing so). To get going with sentence transformers and for downloading/importing a model such as [all-mpnet-bas-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2), there are step by step guides informing users how to use and them and deal with outputs o the Hugging Face website.

#### Dimension Reduction:

At this stage, we would expect to have our data cleaned along with representative embeddings for each document. This next step, explains how we take this high-dimensional embeddings object and then simplify/reduce columns down enough to a more manegable size in order to map our documents onto a semantic space. Documents can then be easily represented as a node and positioned within this abstract space based upon their nature, meaning that those more semantically similar will be situated closer together upon our 2 or 3 dimensional plot, which then forms our landscape.

There are a number of ways one can process an embeddings output to produce either a 2 or 3 dimensional representation of data. Each method has its own merits as well as appropriate use cases, which mostly depend whether the user intends to focus on either the local or global structure of their data. For more on the alternative dimension reduction techniques, the [BERTopic documentation](https://maartengr.github.io/BERTopic/getting_started/dim_reduction/dim_reduction.html) provides some further detail while staying relevant to the subject matter of Topic Modelling in NLP.

Once we have reduced our embeddings, and for the sake of staying consistent to the context of our given example, lets say we have decided to use Uniform Manifold Approximation and Projection(UMAP), as this technique is useful for when being mindful of both the local and global structures within data. The output of this step should have resulted in taking our high dimensional embedding data(often 768 columns or even more) and reduced these values down to just 2 columns so that we can plot them onto our semantic space(conversation landscape plot), using these 2 reduced values as if X and Y coordinates to map each document.

![Grey Colourless Landscape Plot from an AI Landscape Microsoft Project - Q2 FY24](/img/ai_landscape_grey.png){fig-alt="A screenshot of a powerpoint slide showcasing a Topic/Subtopic Breakdown of the Artificial Intelligence Conversation"}

#### Topic Modelling/Clustering:

The last step mentioned is arguably the most important, this is where we will define our corpus of documents and simplify our findings byway of scientific means, in this case Topic Modelling. 

There are a number of algorithms that serve this purpose and are at our disposal at this stage, but the more commonly used clustering techniques are KMeans and HDBSCAN. However, the example we have shown throughout this document uses KMeans, where we define the number of clusters that we would expect to find beforehand (a supervised approach), where-as if we were to opt for HDBSCAN (a semi-supervised approach), we would allow the model to determine how many clusters were present based on some input parameters such as `min_cluster_size` provided by the user. For more on these two techniques and when/how to use them, we can consult the [BERTopic documentation](https://maartengr.github.io/BERTopic/getting_started/clustering/clustering.html#hdbscan) again. It's also worth noting that this step requires a considerable amount of human interpretation, so the user can definitely expect to partake in a considerable amount of trial and error, testing different parameters, determining different model outputs and also checking different a number of clusters per model, with hopes of finding the model of best fit, which accurately represents the given data.

![Segmented Colourful Landscape Plot from an AI Landscape Microsoft Project - Q2 FY24](/img/ai_landscape_coloured.png){fig-alt="A screenshot of a powerpoint slide showcasing a Topic/Subtopic Breakdown of the Artificial Intelligence Conversation" width="675"}

## Downstream Flourishes

With a basic understanding of each step covered, we will now touch on a few potentially beneficial concepts worth grasping that may help us overcome anything else that may occur when working within the Conversation Landscape project domain.

#### Efficient Parameter Tuning:

improving cluster outputs, handling noise etc ...

#### Model Saving & Reusability:

repeat projects etc ...

#### Supporting Data Vizualisation:

alternative plots to support the landscape output ...

#### Continuous Learning:

the concepts are covered, but include some further reading ...

